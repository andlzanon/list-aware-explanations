import inspect

import cornac.models
import pandas as pd

from dataset_experiment.dataset_experiment import DatasetExperiment
from recommenders.evaluation.python_evaluation import map, ndcg_at_k, precision_at_k, recall_at_k
from recommenders.models.cornac.cornac_utils import predict_ranking


class RecommenderSystem:
    def __init__(self, model: cornac.models.Recommender, dataset: DatasetExperiment, remove_seen=True):
        """
        Recommendation class. It generates a list of recommendations for the user and also extract
        metrics. The class uses cornac recommender engines and evaluates with the recommenders library
        :param model: cornac model to run
        :param dataset: dataset experiment to run the model on
        :param remove_seen: True to remove seen items in the recommendation list, False to keep them.
        """
        sig = inspect.signature(model.__init__)
        model_params = {param: getattr(model, param) for param in sig.parameters
                        if hasattr(model, param)}
        model_params = {k: v for k, v in model_params.items() if not isinstance(v, dict)}

        name_params = model_params["name"] + "&"
        del model_params["name"]
        for key in model_params.keys():
            name_params = name_params + str(key) + "=" + str(model_params[key]) + "&"

        self.model_name = name_params[:-1]
        self.model = model
        self.dataset = dataset
        self.remove_seen = remove_seen

    def fit_model(self) -> cornac.models.Recommender:
        """
        Function to fit the model into the training dataset
        :return: fitted model
        """
        print("Training model...")
        if self.dataset.validation is not None:
            self.model.fit(train_set=self.dataset.train, val_set=self.dataset.validation)
        else:
            self.model.fit(train_set=self.dataset.train)

        return self.model

    def recommend_to_user(self, user_id: str, k: int) -> list:
        """
        Function to recommend
        :param user_id: id of the user to generate recommendations to
        :param k: number of recommendations to generate
        :return: list of recommendations items ids in the cornac dataset
        """
        return self.model.recommend(user_id=user_id, k=k,
                                    train_set=self.dataset.train,
                                    remove_seen=self.remove_seen)

    def run_experiment(self, k_list: list, save_results=True) -> dict:
        """
        Run experiment where the recommender system will score all items to all users and extract results.
        This function can also save the score of items for all user, item tuples and the metrics in the file
        system if the flag save_results is set to True.
        :param k_list: list of top k to evaluate
        :param save_results: True if results should be saved in the datasets folder as file, False otherwise
        :return: metrics as dictionary and saved files on file system
        """
        print("Generating Predictions...")
        train_df, _, test_df = self.dataset.load_fold_asdf()
        predictions = predict_ranking(self.model, train_df,
                        usercol=self.dataset.user_column, itemcol=self.dataset.item_column,
                        predcol=self.dataset.rating_column, remove_seen=self.remove_seen)

        if save_results:
            path = self.dataset.path
            if self.dataset.fold_loaded == -1:
                path = path + "/stratified_split/outputs/"
            else:
                path = path + "/folds/" + str(self.dataset.fold_loaded) + "/outputs/"
            path = path + self.model_name + ".csv"
            predictions.to_csv(path, header=True, index=False)

        print("Generating Metrics...")
        metrics = self.__evaluate(predictions=predictions, test_recs=test_df,
                                  k_list=k_list, save_results=save_results)
        return metrics

    def __evaluate(self, predictions: pd.DataFrame, test_recs: pd.DataFrame,
                   k_list: list, verbose=True, save_results=True) -> dict:
        """
        Function that evaluate the predictions generated by the recommender system on top k items
        :param predictions: dataframe containing scores for all possible (user item) tuples, therefore, it has
        three columns: user, item and predicted rating
        :param test_recs: testing set as pandas dataframe
        :param k_list: list of top k recommendations to extract metrics
        :param verbose: True to print the results on console, False otherwise
        :param save_results: True to save the results on file system, false otherwise
        :return: metrics as dictionary
        """
        metrics_dict = {}

        for k in k_list:
            eval_map = map(test_recs, predictions, col_user='userId',
                           col_item='movieId',
                           col_prediction=self.dataset.rating_column, k=k)
            eval_ndcg = ndcg_at_k(test_recs, predictions, col_user='userId',
                                  col_item='movieId',
                                  col_prediction=self.dataset.rating_column, k=k)
            eval_precision = precision_at_k(test_recs, predictions, col_user='userId',
                                            col_item='movieId', col_prediction=self.dataset.rating_column, k=k)
            eval_recall = recall_at_k(test_recs, predictions, col_user='userId',
                                      col_item='movieId', col_prediction=self.dataset.rating_column, k=k)

            metrics_dict[f'''MAP@{k}'''] = eval_map
            metrics_dict[f'''NDCG@{k}'''] = eval_ndcg
            metrics_dict[f'''Precision@{k}'''] = eval_precision
            metrics_dict[f'''Recall@{k}'''] = eval_recall

            if verbose:
                print(f'''--- Metrics ---''')
                print(f'''MAP@{k}: {eval_map}''')
                print(f'''NDCG@{k}: {eval_ndcg}''')
                print(f'''Precision@{k}: {eval_precision}''')
                print(f'''Recall@{k}: {eval_recall}''')

        if save_results:
            path = self.dataset.path
            if self.dataset.fold_loaded == -1:
                path = path + "/stratified_split/results/"
            else:
                path = path + "/folds/" + str(self.dataset.fold_loaded) + "/results/"

            path = path + self.model_name + ".txt"
            with open(path, 'w') as f:
                for key, value in metrics_dict.items():
                    f.write(f'''{key}:{value}\n''')

        return metrics_dict
